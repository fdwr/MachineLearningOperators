<html>
<head>
<style>
body
{
  font-family: Calibri, Segoe UI, sans-serif;
  background-color: #FFFFFF;
}
table tr:nth-child(odd) td{
  background-color: #FFFFFF;
}
table tr:nth-child(even) td{
  background-color: #F8F8FF;
}
td
{
  white-space: pre-wrap;
  vertical-align: top;
}
 
 /* Hide expandable content by default */
.elementwiseOperator { visibility: collapse; }
.activationOperator { visibility: collapse; }
.reorganizationOperator { visibility: collapse; }
.poolingOperator { visibility: collapse; }
.controlFlowOperator { visibility: collapse; }
.reductionOperator { visibility: collapse; }
.normalizationOperator { visibility: collapse; }

/* Show hidden content when the checkbox is checked */
#toggleElementwiseOperators:checked ~ * .elementwiseOperator { visibility: visible; }
#toggleActivationOperators:checked ~ * .activationOperator { visibility: visible; } 
#toggleReorganizationOperators:checked ~ * .reorganizationOperator { visibility: visible; } 
#togglePoolingOperators:checked ~ * .poolingOperator { visibility: visible; } 
#toggleControlFlowOperators:checked ~ * .controlFlowOperator { visibility: visible; } 
#toggleReductionOperators:checked ~ * .reductionOperator { visibility: visible; }
#toggleNormalizationOperators:checked ~ * .normalizationOperator { visibility: visible; }

</style>
</head>
<body>

<h1>Links</h1>
<a href="https://fdwr.github.io/LostOnnxDocs/MlFormulas.html">This document on GitHub</a><br/>
<a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md">ONNX Operators</a><br/>
<a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d12/dml">DirectML</a><br/>
<a href="https://pytorch.org/docs/stable/torch.html">PyTorch Torch interface</a><br/>
<a href="https://pytorch.org/docs/stable/nn.html">PyTorch NN interface</a><br/>

<h1>Operator Equations</h1>

<p>Notation:
<li>Lowercase values are scalar. Uppercase are full tensors. So, x is scalar while X is a tensor, and abs(x) means it returns a scalar while Abs(X) returns a tensor.</li>
<li>Any multiplication signs '*' mean elementwise multiplication (per NumPy, PyTorch, TensorFlow).</li>
<li>Any matrix style compound dot product will explicitly use MatMul() rather than '*' to avoid unacceptable notation ambiguity.</li>
<li>Any addition signs '+' mean elementwise addition, never the horrible notation abuse of concatenation.</li>
<li>All exp() and log() use the natural logarithm 2.718281828 (not base 10 or 2).</li>
<li>The pseudocode below uses a few undefined functions, but their trivial behavior should be obvious to implement. e.g. min, max, ones, zeroes, iif, assert</li>
</p>

<input type="checkbox" id="toggleElementwiseOperators" checked="on"/><label for="toggleElementwiseOperators">Elementwise operators</label><br/>
<input type="checkbox" id="toggleActivationOperators" checked="on"/><label for="toggleActivationOperators">Activation operators</label><br/>
<input type="checkbox" id="toggleReorganizationOperators" checked="on"/><label for="toggleReorganizationOperators">Reorganization operators</label><br/>
<input type="checkbox" id="togglePoolingOperators" checked="on"/><label for="togglePoolingOperators">Pooling operators</label><br/>
<input type="checkbox" id="toggleReductionOperators" checked="on"/><label for="toggleReductionOperators">Reduction operators</label><br/>
<input type="checkbox" id="toggleNormalizationOperators" checked="on"/><label for="toggleNormalizationOperators">Normalization operators</label><br/>
<input type="checkbox" id="toggleControlFlowOperators" checked="on"/><label for="toggleControlFlowOperators">Control flow operators</label><br/>

<table border=1 cellspacing=0 cellpadding=1 style='border-collapse:collapse; border:none'>
 <tr>
  <th style="width:15em">Category</th>
  <th>Name</th>
  <th>Aliases</th>
  <th>DML Name</th>
  <th valign="top">Formula</th>
  <th>Precision</th>
 </tr>
 <tr>
  <td>Generic</td>
  <td>Identity</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Identity">Identity</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IDENTITY or DML_ACTIVATION_IDENTITY</td>
  <td><code>Identity(X) = X</code></td>
  <td>Exact</td>
 </tr>
 <tr>
  <td>Input</td>
  <td>Constant</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Constant">Constant</a></td>
  <td>NA just provide the tensor data</td>
  <td><code>Constant() = Value</code></td>
  <td>Exact</td>
 </tr>
 <tr>
  <td>Input</td>
  <td>Constant Of Shape</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#ConstantOfShape">ConstantOfShape</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IDENTITY with zero strides to broadcast</td>
  <td><code>ConstantOfShape(scalarTensorValue, inputOfDesiredShape) = Expand(scalarTensorValue, Shape(inputOfDesiredShape))</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Input (Deleted)</td>
  <td>Constant Fill</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.2.1/docs/Operators.md#ConstantFill">ConstantFill</a></td>
  <td>Use DML_OPERATOR_ELEMENT_WISE_IDENTITY with zero strides for broadcasting</td>
  <td><code>function ConstantFill(dtype, extra_shape, input_as_shape, shape, value; inputOfDesiredShape)
    assert(!exists(shape) || !input_as_shape)
    output.dataType = dtype
    output.shape = if exists(shape) then shape else Shape(inputOfDesiredShape) + extra_shape
    for each element in output tensor
        element = value
    endfor
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr>
  <td>Input (Deleted)</td>
  <td>Constant Fill</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.3.0/docs/Operators.md#GivenTensorFill">GivenTensorFill</a></td>
  <td>NA experimental</td>
  <td>unknown</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Add</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Add">Add</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ADD</td>
  <td><code>f(x, y) = x + y</code></td>
  <td>1 ULP</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Subtract</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sub">Sub</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_SUBTRACT</td>
  <td><code>f(x, y) = x - y</code></td>
  <td>1 ULP</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Multiply</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Mul">Mul</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_MULTIPLY</td>
  <td><code>f(x, y) = x * y</code></td>
  <td>1 ULP</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Divide</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Div">Div</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_DIVIDE</td>
  <td><code>f(x, y) = x / y</code></td>
  <td>1 ULP</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Square root</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sqrt">Sqrt</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_SQRT</td>
  <td><code>f(x) = sqrt(x)</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Reciprocal</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Reciprocal">Reciprocal</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_RECIP</td>
  <td><code>f(x) = 1 / x</code></td>
  <td>1 ULP</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Power</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pow">Pow</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_POW</td>
  <td><code>f(x, exponent) = pow(x, exponent)</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Expnonent</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Exp">Exp</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_EXP</td>
  <td><code>f(x) = expₑ(x)</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Logarithm</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Log">Log</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOG</td>
  <td><code>f(x) = logₑ(x)</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Absolute</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Abs">Abs</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ABS</td>
  <td><code>f(x) = abs(x)</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Negative</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Neg">Neg</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IDENTITY with scale = -1</td>
  <td><code>f(x) = -x</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Ceiling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Ceil">Ceil</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_CEIL</td>
  <td><code>f(x) = ceil(x)</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Floor</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Floor">Floor</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_FLOOR</td>
  <td><code>f(x) = floor(x)</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Clamp</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Clip">Clip</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_CLIP</td>
  <td><code>f(x) = clamp(x, min, max)</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Error Function</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Erf">Erf</a></td>
  <td>---</td>
  <td><code>erf(x) = 1/sqrt(pi) * integrate(i = -x to x, e ^ -(i^2))</code>
or: <code>erf(x) = 2/sqrt(pi) * integrate(i = 0 to x, e ^ -(i^2))</code>
<code>
double f(double x)
{
    // Polynomial approximation constants.
    double a1 =  0.254829592;
    double a2 = -0.284496736;
    double a3 =  1.421413741;
    double a4 = -1.453152027;
    double a5 =  1.061405429;
    double p  =  0.3275911;

    // Save the sign of x.
    int sign = 1;
    if (x &lt; 0) sign = -1;
    x = fabs(x);

    // Approximate the formula A&S 7.1.26:
    // 2/sqrt(pi) * integrate(i = 0 to x, e ^ -(i^2))
    double t = 1.0/(1.0 + p*x);
    double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1) * t*expₑ(-x*x);
    return sign * y;
}</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Is Not a Number</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#IsNaN">IsNan</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IS_NAN</td>
  <td>f(x) = isnan(x)
      where isnan(float32 x): return (reinterpret_cast(x, uint32_t) & 0x7FFFFFFF) &gt; 0x7F800000.
      Any float32 value with all 1's for exponent and a nonzero mantissa is <a href="https://en.wikipedia.org/wiki/NaN">NaN</a>. The sign is ignored.
      e.g. s1111111 10000000 0000000 00000001 - float32
      e.g. s1111100 00000001 - float16</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Is Infinity</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.5.0/docs/Operators.md#IsInf">IsInf</a></td>
  <td>---</td>
  <td><code>output.shape = input.shape
for every coordinate in input tensor
    hasExpectedSign = iif(input[coordinate] &gt; 0, detect_positive, detect_negative)
    output[coordinate] = isinf(input[coordinate]) && hasExpectedSign
    // or test that all exponent bits are one and all mantissa bits are 0:
    // (input value & 0x7FFFFFFF) == 0x7F800000
endfor
</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Sign</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Sign">Sign</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_SIGN</td>
  <td><code>function Sign(x)
    if x == 0 then 0
    elif x &gt; 0 then 1
    elif x &lt; 0 then -1
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math (Deleted)</td>
  <td>Scale Signal</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.0/docs/Operators.md#Scale">Scale</a></td>
  <td>Same as Mul with broadcasting</td>
  <td>F(X) = Mul(X, scale)
      or: f(x) = x * scale</td>
  <td>1 ULP</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math (Deleted)</td>
  <td>Image Scaler</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.3.0/docs/Operators.md#ImageScaler">ImageScaler</a></td>
  <td>DML_OPERATOR_VALUE_SCALE_2D</td>
  <td>F(X) = Add(Mul(X, scale), Unsqueeze(biasTensor, [0, 2, 3])) // reshape bias to [1,C,1,1]
      or: f(x, scale, bias) = x * scale + bias</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Logical</td>
  <td>Greater Than</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Greater">Greater</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN</td>
  <td>f(x, y) = (x &gt; y)</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Logical</td>
  <td>Less Than</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Less">Less</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN</td>
  <td>f(x, y) = (x &lt; y)</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Logical</td>
  <td>Equals</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Equal">Equal</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOGICAL_EQUALS</td>
  <td>f(x, y) = (x == y)</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Logical</td>
  <td>Not</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Not">Not</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOGICAL_NOT</td>
  <td>f(x) = !x</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Logical</td>
  <td>And</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#And">And</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOGICAL_AND</td>
  <td>f(x, y) = x &amp;&amp; y</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Logical</td>
  <td>Or</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Or">Or</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOGICAL_OR</td>
  <td>f(x, y) = x || y</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Logical</td>
  <td>Xor</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Xor">Xor</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_LOGICAL_XOR</td>
  <td>f(x, y) = !!x xor !!y</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Sine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sin">Sin</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_SIN</td>
  <td>f(x) = sin(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Cosine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Cos">Cos</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_COS</td>
  <td>f(x) = cos(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Tangent</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Tan">Tan</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_TAN</td>
  <td>f(x) = tan(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Arcsine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Asin">Asin</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ASIN</td>
  <td>f(x) = asin(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Arccosine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Acos">Acos</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ACOS</td>
  <td>f(x) = acos(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Arctangent</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Atan">Atan</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ATAN</td>
  <td>f(x) = atan(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Hyperbolic Sine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Sinh">Sinh</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_SINH</td>
  <td>f(x) = sinh(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Hyperbolic Cosine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Cosh">Cosh</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_COSH</td>
  <td>f(x) = cosh(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Hyperbolic Tangent</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Tanh">Tanh</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_TANH</td>
  <td>f(x) = tanh(x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Hyperbolic Arccosine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Acosh">Acosh</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ACOSH</td>
  <td>f(x) = arccosh(x)
      or: logₑ(x + sqrt(x * x - 1))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Hyperbolic Arccosine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Asinh">Asinh</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ASINH</td>
  <td>f(x) = arcsinh(x)
      or: logₑ(x + sqrt(x * x + 1))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>Hyperbolic Arccosine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Atanh">Atanh</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ATANH</td>
  <td>f(x) = arctanh(x)
      or: logₑ((1 + x) / (1 - x)) / 2</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Trigonometric</td>
  <td>CosineGrad</td>
  <td>Composed</a></td>
  <td>Composed</td>
  <td>F(Dx, X) = Mul(Sin(X), Dx)
      f(dx, x) = sin(x) * dx</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Reduction</td>
  <td>Sum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sum">Sum</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ADD via repeated inputs</td>
  <td>f(x, y, z…) = x + y + z…</td>
  <td>&lt; N-1 ULP</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Reduction</td>
  <td>Mean</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Mean">Mean</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_MEAN via repeated inputs</td>
  <td>f(x, y, z…) = (x + y + z…) / n</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Reduction</td>
  <td>Maximum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Max">Max</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_MAX via repeated inputs</td>
  <td>f(x, y, z…) = max(x, y, z…)</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math</td>
  <td>Threshold</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Max">Max</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_THRESHOLD</td>
  <td>F(X, minValue) = Max(X, minValue)
      or: f(x) = max(x, minValue)
      notes: Not equivalent to ThresholdedRelu.</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Reduction</td>
  <td>Minimum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Min">Min</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_MIN via repeated inputs</td>
  <td>f(x, y, z…) = min(x, y, z…)</td>
  <td>Exact</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Quantization</td>
  <td>Quantize Linear</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#QuantizeLinear">QuantizeLinear</a><br/>com.microsoft QuantizeLinear</td>
  <td>DML_OPERATOR_ELEMENT_WISE_QUANTIZE_LINEAR</td>
  <td>f(input:float32, scale:float32, zero_point:int32)
      return output:uint8 = clamp(round(input / scale) + zero_point, 0, 255)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="elementwiseOperator">
  <td>Elementwise Math Quantization</td>
  <td>Dequantize Linear</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#DequantizeLinear">DequantizeLinear</a><br/>com.microsoft DequantizeLinear</td>
  <td>DML_OPERATOR_ELEMENT_WISE_DEQUANTIZE_LINEAR</td>
  <td>f(input:uint8, scale:float32, zero_point:float32)
      return output:float32 = (input - zero_point) * scale</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Sigmoid</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Sigmoid">Sigmoid</a></td>
  <td>DML_OPERATOR_ACTIVATION_SIGMOID</td>
  <td>F(X) = Reciprocal(Add(1, Exp(Neg(X))))
      or: f(x) = 1 / (1 + expₑ(-x))
      or: f(x) = expₑ(x) / (1 + expₑ(x))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Hard Sigmoid</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#HardSigmoid">HardSigmoid</a></td>
  <td>DML_OPERATOR_ACTIVATION_HARD_SIGMOID</td>
  <td>F(X) = Clip(Add(Mul(alpha, X), beta), 0, 1))
      or: f(x) = max(0, min(alpha * x + beta, 1))
      defaults: alpha = 0.2, beta = 0.5</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Hyperbolic Tangent</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Tanh">Tanh</a></td>
  <td>DML_OPERATOR_ACTIVATION_TANH</td>
  <td>F(X) = Div(Sub(1, Exp(Mul(X, -2)), Add(1, Exp(X, -2)))
      or: f(x) = (1 - expₑ(-2 * x))/(1 + expₑ(-2 * x))
      or: f(x) = 2 /(1 + expₑ(-2 * x)) - 1
      or: f(x) = (expₑ(x) - expₑ(-x)) / (expₑ(x) + expₑ(-x))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Scaled Hyperbolic Tangent</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.3.0/docs/Operators.md#ScaledTanh">ScaledTanh</a></td>
  <td>DML_OPERATOR_ACTIVATION_SCALED_TANH</td>
  <td>F(X, alpha, beta) = Mul(Tanh(Mul(X, beta)), alpha)
      or: f(x, alpha, beta) = alpha * tanh(beta * x)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Clamp Positive (Rectified Linear Unit)</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Relu">Relu</a></td>
  <td>DML_OPERATOR_ACTIVATION_RELU</td>
  <td>F(X) = Max(X, 0)
      or: f(x) = max(x, 0)
      or: if x &gt;= 0 then x else 0</td>
  <td>Exact</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Leaky Rectified Linear Unit</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#LeakyRelu">LeakyRelu</a></td>
  <td>DML_OPERATOR_ACTIVATION_LEAKY_RELU</td>
  <td>F(X) = Where(Less(X, 0), Mul(X, alpha), X)
      or: f(x, alpha) = if x &gt;= 0 then x else alpha * x</td>
  <td>&lt;= Mul precision</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Parameterized Rectified Linear Unit</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#PRelu">PRelu</a></td>
  <td>DML_OPERATOR_ACTIVATION_PARAMETERIZED_RELU</td>
  <td>F(X) = Where(Less(X, 0), Mul(X, Slope), X)
      or: f(x, slope) = if x &gt;= 0 then x else slope * x
      PRelu and LeakyRelu are identical, except one slope is an input tensor and one slope is a constant.</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Thresholded Rectified Linear Unit</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ThresholdedRelu">ThresholdedRelu</a></td>
  <td>DML_OPERATOR_ACTIVATION_THRESHOLDED_RELU</td>
  <td>F(X, alpha = 1) = Where(Greater(X, alpha), X, 0)
      f(x) = if x &gt; alpha then x else 0</td>
  <td>Exact</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Exponential Linear Unit</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Elu">Elu</a></td>
  <td>DML_OPERATOR_ACTIVATION_ELU</td>
  <td>F(X, alpha = 1) = Where(Less(X, 0), Mul(Sub(Exp(X), 1), alpha))
      or: F(X, alpha = 1) = Add(Clip(X, 0, inf), Clip(Mul(Sub(Exp(X), 1), alpha), -inf, 0))
      or: f(x, alpha = 1) = if x &gt;= 0 then x else alpha * (expₑ(x) - 1)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Scaled Exponential Linear Unit</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Selu">Selu</a></td>
  <td>DML_OPERATOR_ACTIVATION_SCALED_ELU</td>
  <td>F(X, alpha = 1.6732, gamma = 1.0507) = Mul(Elu(X, alpha), gamma)
      or: f(x, alpha = 1.6732, gamma = 1.0507):
      if x &gt; 0 then gamma * x
      else gamma * alpha * (expₑ(x) - 1)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator normalizationOperator">
  <td>Activation / Normalization</td>
  <td>Soft Maximum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softmax">Softmax</a></td>
  <td>DML_OPERATOR_ACTIVATION_SOFTMAX</td>
  <td>Raise all elements to e, and divide all the elements in each batch by that batch's sum.
<code>function SoftMax2D(Input; Output; axis):
  MaxInput = ReduceMax(Input, axes=[1], keepdims=1)
  ExpInput = Exp(Input - MaxInput)
  ReducedExpInput = ReduceSum(ExpInput, axis=[1], keepdims=1)
  Output = ExpInput / ReducedExpInput
endfunction

function F(Input; Output; axis):
  FlattenedInput = Flatten(Input, axis) // Flatten to 2D
  NormalizedInput = SoftMax2D(FlattenedInput)
  Output = Reshape(NormalizedInput, Shape(X))
endfunction
</code>

    or per batch: f(x) = expₑ(x) / sum(expₑ(X))
    or per batch for more numerical stability: expₑ(x - max(X)) / sum(expₑ(x - max(X)))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Log Soft Maximum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#LogSoftmax">LogSoftmax</a></td>
  <td>DML_OPERATOR_ACTIVATION_LOG_SOFTMAX</td>
  <td>F(Input, axis) = Log(Softmax(Input, axis))
  or: f(x) = logₑ(expₑ(x - max(X)) / sum(expₑ(X - max(X))))
  or: (x - max(X)) - logₑ(x - max(X))))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Hard Maximum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Hardmax">Hardmax</a></td>
  <td>DML_OPERATOR_ACTIVATION_HARDMAX</td>
  <td>f(x) = if x_i == max(X) then 1 else 0
      *but only for first element along that axis</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Soft Sign</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softsign">Softsign</a></td>
  <td>DML_OPERATOR_ACTIVATION_SOFTSIGN</td>
  <td>F(X) = Div(X, Add(Abs(X), 1))
      or: f(x) = x / (1 + abs(x))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Softplus</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Softplus">Softplus</a></td>
  <td>DML_OPERATOR_ACTIVATION_SOFTPLUS</td>
  <td>F(X) = Log(Add(Exp(x), 1))
      or: f(x) = logₑ(1 + expₑ(x))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Affine</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.3.0/docs/Operators.md#Affine">Affine</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IDENTITY
  with scale and bias or DML_OPERATOR_ACTIVATION_LINEAR</td>
  <td>F(X, alpha, beta) = Add(Mul(X, alpha), beta)
  or: f(x, alpha, beta) = alpha * x + beta</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation</td>
  <td>Symmetric signal shift</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#shrink">Shrink</a></td>
  <td>DML_OPERATOR_ACTIVATION_SHRINK</td>
  <td>f(x, lambda, bias):
      if x &lt; -lambda then y = x + bias
      elif x &gt; lambda then y = x - bias
      else y = 0</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="activationOperator">
  <td>Activation (Deleted)</td>
  <td>Parametric Softplus</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.3.0/docs/Operators.md#ParametricSoftplus">ParametricSoftplus</a></td>
  <td>DML_OPERATOR_ACTIVATION_PARAMETRIC_SOFTPLUS</td>
  <td>F(X, alpha, beta) = Mul(alpha, Softplus(Mul(beta, X))).
      or: f(x, alpha, beta) = alpha * logₑ(1 + expₑ(beta * x))
  </td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Generation Random</td>
  <td>Random Normal</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#RandomNormal">RandomNormal</a></td>
  <td>---</td>
  <td>f(scale, mean) = <a href="https://en.wikipedia.org/wiki/Marsaglia_polar_method">MarsagliaPolarTransform</a>(random(), random()) * scale + mean</td>
  <td>Unpredictable</td>
 </tr>
 <tr>
  <td>Generation Random</td>
  <td>Random Normal Like</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#RandomNormalLike">RandomNormalLike</a></td>
  <td>---</td>
  <td>f(scale, mean) = <a href="https://en.wikipedia.org/wiki/Marsaglia_polar_method">MarsagliaPolarTransform</a>(random(), random()) * scale + mean</td>
  <td>Unpredictable</td>
 </tr>
 <tr>
  <td>Generation Random</td>
  <td>Random Uniform</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#RandomUniform">RandomUniform</a></td>
  <td>---</td>
  <td>f(low, high): // see MT19937
      range = high - low // note inclusive end
      if integer: return (rand() % (range+1) + low 
      if float: (rand() / randmax) * range + low</td>
  <td>Unpredictable</td>
 </tr>
 <tr>
  <td>Generation Random</td>
  <td>Random Uniform Normal</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#RandomUniformLike">RandomUniformLike</a></td>
  <td>---</td>
  <td>f(low, high): // see MT19937
      range = high - low // note inclusive end
      if integer: return (rand() % (range+1) + low 
      if float: (rand() / randmax) * range + low</td>
  <td>Unpredictable</td>
 </tr>
 <tr>
  <td>Generation Random</td>
  <td>Random Multinomial</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Multinomial">Multinomial</a></td>
  <td>---</td>
  <td>TODO:</td>
  <td>Unpredictable</td>
 </tr>
 <tr>
  <td>Generation Matrix Multiplication</td>
  <td>Diagonal Matrix</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#EyeLike">EyeLike</a></td>
  <td>DML_OPERATOR_DIAGONAL_MATRIX</td>
  <td>Notes: set 1's all along diagonal. In other words, all output[i, i+k] = 1, and every other element = 0.<code>
output.shape = input.shape
output.type = if exists(dtype) then dtype else input.type
function EyeLike(input, k=0):
  for each coordinate in output tensor
    output[coordinate] = if coordinate.h + k == coordinate.w then 1 else 0
  endfor
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr>
  <td>Matrix Multiplication</td>
  <td>Generic Matrix Multiplication</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Gemm">Gemm</a></td>
  <td>DML_OPERATOR_MATRIX_GEMM</td>
  <td>F(A, B, C; alpha, beta, transA, transB; Y):
      A2 = If(transA, Transpose(A), A)
      B2 = If(transB, Transpose(B), B)
      Y = Add(Mul(alpha, MatMul(A2, B2)), Mul(beta, C))</td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Matrix Multiplication</td>
  <td>Matrix Multiplication</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#MatMul">MatMul</a></td>
  <td>DML_OPERATOR_MATRIX_GEMM</td>
  <td>for i=0..&lt;h do for j=0..&lt;w do y[i,j] = dot(A[i,0..w], B[0..h,j])
      It's essentially a ReduceSum(Mul(A.row, B.column)) per output element.
      notes: A and B can be 1D vectors, which are treated as [1,W] and [H,1] matrices. A and B can have batch count dimensions, where each 2D matrix is multiplied separately. The batch count can be broadcast too.</td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Matrix Multiplication</td>
  <td>Convolve</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Conv">Conv</a></td>
  <td>DML_OPERATOR_CONVOLUTION with DML_CONVOLUTION_MODE_CROSS_CORRELATION, DML_CONVOLUTION_DIRECTION_FORWARD</td>
  <td>Every output element is the convolution of the filter with the corresponding input elements.
  <code>out[j] = (x[i]*w[0]) + (x[i+1]*w[1]) + (x[i+2]*w[2]) + ... + (x[i+k]*w[k]) + b</code>
      notes: 'steps' affects the size of steps over the input.
      'dilations' affects the step of the filter, as if the filter had been resized with interceding zeroes between elements.
      A dilation of 1 means no change (1:1 with filter), whereas dilation of 2 inserts lines of zeros between every filter line.
      'pads' are not actually added to the input, just virtually treated as if zeros. <a href="https://github.com/vdumoulin/conv_arithmetic">vdumoulin convolution diagrams</a>
<code>
function Convolve(input, filterWeights, dilations, strides, kernel_shape, pads; output)
  startPads = pads[0..pads.size/2]
  endPads = pads[pads.size/2..pads.size]
  // todo: compute output size
  // output.shape = (input.shape + startPads + endPads) // todo: consider strides and kernel size
  for each outputCoordinate in output
    output[outputCoordinate] = ConvolveKernel(input, filterWeights, outputCoordinate * strides - startPads, dilations)
  endfor
endfunction

function ConvolveKernel(input, filterWeights, firstInputCoordinate, dilations)
  // 2D example only todo:Figure out what 'group' does and what 'M' is?
  result = 0
  // todo: How do 'M' and 'C' factor into this?
  for y=0..&lt;filterWeights.shape[2]
    for x=0..&lt;filterWeights.shape[3]
      inputCoordinate = firstInputCoordinate + ([y,x] * dilations)
      if (input.contains(inputCoordinate)) // check coordinates within tensor
        result += filterWeights[y,x] * input[inputCoordinate]
      endif
    endfor // x
  endfor // y
  return result
endfunction
</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Matrix Multiplication</td>
  <td>Convolve Tranposed</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ConvTranspose">ConvTranspose</a></td>
  <td>DML_OPERATOR_CONVOLUTION with DML_CONVOLUTION_MODE_CROSS_CORRELATION, DML_CONVOLUTION_DIRECTION_BACKWARD</td>
  <td>todo: Here be dragons.
      questions: What is the difference between CONVOLUTION vs CORRELATION enum, and FORWARD vs BACKWARD?</td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Data Conversion</td>
  <td>Cast</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Cast">Cast</a></td>
  <td>DML_OPERATOR_CAST</td>
  <td>f(x) = cast(x)</td>
  <td>&lt; 1 ULP</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Transpose</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Transpose">Transpose</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IDENTITY with new TENSOR_DESC that flips via permuted strides</td>
  <td>Reorder axes, such as X Y -&gt; Y X, or X Y Z -&gt; Z X Y.
<code>
function Transpose(input, perm):
  N = input.rank
  assert(perm.size == input.rank)
  for i=0..&lt;N do output.shape[i] = input.shape[perm[i]]
  for each inputCoordinate in input
    for i=0..&lt;N do outputCoordinate[i] = inputCoordinate[perm[i]]
    output[outputCoordinate] = input[inputCoordinate]
  endfor
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Broadcast</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.3.0/docs/Operators.md#Expand">Expand</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IDENTITY with TENSOR_DESC using zero strides along broadcast dimension</td>
  <td>Broadcast any single size dimensions up to the output dimension counts. Similar to <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.broadcast_to.html">NumPy broadcast_to</a>.
<code>
function Broadcast(input, shape)
  output.shape = BroadcastShape(input.shape, shape)
  N = output.rank
  inputShape = PadLeadingValues(input.shape, N, 1)
  for each outputCoordinate in output
    for i=0..&lt;N do inputCoordinate[i] = iif(inputShape[i] &gt; 1), outputCoordinate[i], 0)
    output[outputCoordinate] = inputData[inputCoordinate]
  endfor
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>BroadcastShape</td>
  <td>No ONNX equivalent</a></td>
  <td>NA</td>
  <td>Compute the broadcasted output shape (1D tensor) from multiple input shapes, where any single size dimensions are stretched to the output dimension size.
<code>
function BroadcastShape(shapes...)
  N = 0
  for each shape in shapes do N = max(N, shape.size) // Determine the largest rank in shapes
  broadcastedShape = ones(N) // [1,1,...]
  for each shape in shapes // Take the max of all shape dimensions.
    paddedShape = PadLeadingValues(shape, N, 1) // Right align. e.g. N=4 with [H,W] -&gt; [1,1,H,W]
    for i=0..&lt;N do
      assert(paddedShape[i] == broadcastedShape[i] || paddedShape[i] == 1 || broadcastedShape[i] == 1))
      broadcastedShape[i] = max(broadcastedShape[i], paddedShape[i])
    endfor
  endfor
  return broadcastedShape
endfunction

function PadLeadingValues(shape, paddedSize, padValue)
  // Right align. e.g. shape=[H,W], paddedSize=4, padValue=1 -&gt; [1,1,H,W]
  paddedSize = max(padddedSize, shape.size)
  padCount = paddedSize - shape.size
  for i=0..&lt;paddedSize
    paddedShape[i] = i &lt; padCount ? padValue : shape[i - padCount]
  endfor
  return paddedShape
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Tile</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Tile">Tile</a></td>
  <td>DML_OPERATOR_TILE</td>
  <td>Repeat entire tensor along each axis by repeat counts.
<code>
function Tile(input, repeats)
  N = input.rank
  assert(repeats.size == input.rank)
  for i=0..&lt;N do assert(repeats[i] &gt; 0)
  output.shape = input.shape * repeats // elementwise multiply per axis
  for each outputCoordinate in output
    for i=0..&lt;N do inputCoordinate[i] = outputCoordinate[i] % input.shape[i]
    output[outputCoordinate] = inputData[inputCoordinate]
  endfor
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Split</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Split">Split</a></td>
  <td>DML_OPERATOR_SPLIT</td>
  <td>Split input into multiple output tensors.</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Slice</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Slice">Slice</a></td>
  <td>DML_OPERATOR_SLICE</td>
  <td>Crop the tensor to the given ranges for each axis.
<code>function Slice(input, starts, ends, axes, steps)
  N = input.rank
  if axes.empty then axes = arange(0, N-1) // [0,1,2,...]
  if starts.empty then starts = zeroes(N) // [0,0,0,...]
  if ends.empty then ends = input.shape
  if steps.empty then steps = ones(N) // [1,1,1,...]
  assert(axes.size == input.rank || axes.size == 0)
  assert(starts.size == axes.size)
  assert(ends.size == axes.size)
  assert(steps.size == axes.size)
  starts = max(starts, zeroes(N))
  ends = min(ends, input.shape)
  ends = max(ends, starts)

  for i=0..&lt;N do output.shape[i] = ceil((ends[i] - starts[i]) / steps[i]) // negative steps unhandled!
  for each outputCoordinate in output
    for i=0..&lt;N do inputCoordinate[i] = outputCoordinate[i] * steps[i] + starts[i]
    output[outputCoordinate] = inputData[inputCoordinate]
  endfor
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization (Deleted)</td>
  <td>Crop</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#experimental-crop">Crop</a> (ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Slice">Slice</a> subset)?</td>
  <td>DML_OPERATOR_SLICE</td>
  <td>Crop the tensor to the given ranges for each axis. Crop is confusing and redundant. Just use Slice.</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Contenate</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Concat">Concat</a></td>
  <td>DML_OPERATOR_JOIN</td>
  <td>Combine multiple tensors into large output tensor. e.g. {1,2,3} with {4,5} -&gt; {1,2,3,4,5}
<code>function Concat(inputs, axis)
  sizesAlongAxis = []
  for each input in inputs
    sizesAlongAxis.append(input.shape[axis])
  endfor
  outputOffsets = CumSum(axisSizes)
  for each inputIndex from 0 up to inputs.count
    input = inputs[inputIndex]
    outputOffset = outputOffset[inputIndex]
    for each index from 0 up to axis
      output[..., outputOffset + index, ...] = input[..., index, ...]
    endfor
  endfor
</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Gather</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Gather">Gather</a></td>
  <td>DML_OPERATOR_GATHER</td>
  <td>TODO:</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Gather Elements</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#GatherElements">GatherElements</a><br/><a href="https://pytorch.org/docs/stable/torch.html#torch.gather">torch.gather</a></td>
  <td>---</td>
  <td>Return output tensor the same size as indices, filling with values from input indexed along the axis by indices.
<code>
output = input
for each coordinate in indices tensor
    inputCoordinate = coordinate
    inputCoordinate[axis] = indices[coordinate]
    output[coordinate] = input[inputCoordinate]
endfor

output[i][j][k] = input[ index[i][j][k] ][j][k]  # if dim == 0
output[i][j][k] = input[i][ index[i][j][k] ][k]  # if dim == 1
output[i][j][k] = input[i][j][ index[i][j][k] ]  # if dim == 2
</code>

e.g.
<code>
input = [1,2,3,4,5,6]
indices = [0,0,1,5]
axis = 0
output = [1,1,2,6]

e.g.
input = [[1,2],[3,4],[5,6]]
indices = [[0,0],[1,0],[1,1]]
axis = 1
output = [[1,1], [4,3], [6,6]]
</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data reorganization</td>
  <td>Scatter Elements</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.6.0/docs/Operators.md#ScatterElements">ScatterElements</a><br/><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_">torch.tensor.scatter_</a></td>
  <td>---</td>
  <td>Opposite of gather. Write values from updates into data at the given indices. If two output element indices overlap, there is no documented winner, but last write wins in practice.
<code>
output = input
for each coordinate in indices tensor
    outputCoordinate = coordinate
    outputCoordinate[axis] = indices[coordinate]
    output[outputCoordinate] = updates[coordinate]
endfor

output[ index[i][j][k] ][j][k] = input[i][j][k]  # if dim == 0
output[i][ index[i][j][k] ][k] = input[i][j][k]  # if dim == 1
output[i][j][ index[i][j][k] ] = input[i][j][k]  # if dim == 2
</code>

<code>
      e.g.
      data = [[1, 2, 3, 4, 5]] // data == input
      indices = [[1, 3]]
      updates = [[11, 21]]
      axis = 0
      output = [[1, 11, 3, 21, 5]]
</code>
  </td>
  <td>Exact</td>
</tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Pad</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pad">Pad</a></td>
  <td>DML_OPERATOR_PADDING</td>
  <td>Inflate the input with zeroes on the edges</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Space To Depth</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#SpaceToDepth">SpaceToDepth</a></td>
  <td>DML_OPERATOR_SPACE_TO_DEPTH</td>
  <td>Rearrange blocks of elements.
<code>
channelCountDivBlockCount = channelCount / (blockSize * blockSize);
inputIndices = [
    outputIndices.batch,
    outputIndices.channel % channelCountDivBlockCount,
    (outputIndices.channel / channelCountDivBlockCount) / blockSize + (outputIndices.height * blockSize),
    (outputIndices.channel / channelCountDivBlockCount) % blockSize + (outputIndices.width  * blockSize)
    ]

output[outputIndices] = input[inputIndices];</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Depth To Space</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#DepthToSpace">DepthToSpace</a></td>
  <td>DML_OPERATOR_DEPTH_TO_SPACE</td>
  <td>Rearrange blocks of elements.
<code>
channelCountDivBlockCount = channelCount / (blockSize * blockSize);
outputIndices = [
    inputIndices.batch,
    inputIndices.channel % channelCountDivBlockCount,
    (inputIndices.channel / channelCountDivBlockCount) / blockSize + (inputIndices.height * blockSize),
    (inputIndices.channel / channelCountDivBlockCount) % blockSize + (inputIndices.width  * blockSize)
    ]

output[outputIndices] = input[inputIndices];</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Shape</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Shape">Shape</a></td>
  <td>NA, just read the TENSOR_DESC dimensions</td>
  <td>Return the dimensions of the tensor as a 1D tensor.
      F(input): return input.shape
  </td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Element Count</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Size">Size</a></td>
  <td>NA, just compute the number of TENSOR_DESC elements</td>
  <td>Return the element count. ReduceProd(Shape(X), keepdims=0).
      note: Size is unfortunately named, inconsistently with Resize10 which accepts separate dimensions. If you want the sizes of the tensor (N C H W) rather than just the total element count, called Shape instead.</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Reshape</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Reshape">Reshape</a></td>
  <td>NA, no actual data change, just update the TENSOR_DESC</td>
  <td>Return tensor with a different view of the data, like a reinterpret cast using new dimensions that are element-count compatible.</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Reshape To 2D</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Flatten">Flatten</a></td>
  <td>NA, no actual data change, just update the TENSOR_DESC</td>
  <td>Reinterpret the view of the tensor, reducing the dimensions from N to 2 (e.g. [1,2,3,4,5] with a split at axis 3 -&gt; [1*2*3,4*5] -&gt; [6,20]).

<code>
aggregate Flatten(input, axis)
    InputShape = Shape(input)
    ShapeFrontHalf = Slice(InputShape; ends=axis)
    ShapeBackHalf = Slice(InputShape; starts=axis)
    NewShape = Concat(axis=0, ReduceProd(ShapeFrontHalf), ReduceProd(ShapeBackHalf))
    Output = Reshape(input, NewShape)
endaggregate
</code>

<code>
function Flatten(input, axis)
  output = input
  inputShape = input.shape
  output.shape = join(reduceprod(inputShape[0..axis]), reduceprod(inputShape[axis..oldShape.size]))
  return output
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Reshape Removing 1's</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Squeeze">Squeeze</a></td>
  <td>NA, just rearrange the TENSOR_DESC</td>
  <td>Reinterpret the view of the tensor, removing 1's for deletable axes.

<code>
function ReshapeDeletingOnes(input, axes)
  outputShape = input.shape
  if axes.empty
    axes = arange(0, outputShape.size)
  else // !axes.empty
    for i in axes do assert(outputShape[i] == 1)
    axes = removeDupes(sort(axes))
  endif
  axes = reverse(axes)
  for i in axes // work from back to front
    if outputShape[i] == 1
      outputShape.deleteAt(i)
    endif
  endfor
  output = input
  output.shape = outputShape
  return output
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Reshape Inserting 1's</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Unsqueeze">Unsqueeze</a></td>
  <td>NA, just rearrange the TENSOR_DESC</td>
  <td>Reinterpret the view of the tensor, filling in 1's for newly inserted axes.

<code>
function ReshapeInsertingOnes(input, axes)
  output = input
  outputShape = input.shape
  axes = removeDupes(sort(axes))
  for i in axes
    outputShape.insertAt(i, 1)
  endfor
  output.shape = outputShape
  return output
endfunction</code></td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data reorganization mapping</td>
  <td>One Hot Along Axis</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#OneHot">OneHot</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_ONE_HOT</td>
  <td>Set all elements to 'off' values, then set one element to 'on' value along specified axis using index offset.</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data Reorganization</td>
  <td>Top K Sorted Selection</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#TopK">TopK</a></td>
  <td>DML_OPERATOR_TOP_K</td>
  <td>TopK(X, K, axis) = Slice(SortDecreasing(X, axis), starts=0, ends=K, axes=axis)</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data reorganization selection</td>
  <td>Select elementwise</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Where">Where</a></td>
  <td>DML_OPERATOR_ELEMENT_WISE_IF</td>
  <td>f(b, x, y) = if b then x else y
      notes: A conditional per-element if statement. Can be used to implement composites that use logical operators (e.g. PRelu).
  </td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data reorganization selection</td>
  <td>Join selected slices</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#Compress">Compress</a></td>
  <td>---</td>
  <td>A conditional slice/join along a specific axis. Has utterly nothing to do with data compression, despite the confusing name.</td>
  <td>Exact</td>
 </tr>
 <tr class="reorganizationOperator">
  <td>Data reorganization <a href="https://github.com/onnx/onnx/pull/1882">(Reverted)</a></td>
  <td>Reverse axes</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/pull/1804">Reverse</a></td>
  <td>---</td>
  <td>Reverse all the elements along the given axes.
<code>
output.shape = input.shape
for every inputCoordinate in input tensor
    // Flip the coordinate along all applicable axes
    outputCoordinate = inputCoordinate
    for each axis in axes
        outputCoordinate[axis] = input.shape[axis] - outputCoordinate[axis]
    endfor

    output[outputCoordinate] = input[inputCoordinate]
endfor
</code></td>
  <td>Exact</td>
 </tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Global Average Pooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#GlobalAveragePool">GlobalAveragePool</a></td>
  <td>DML_OPERATOR_AVERAGE_POOLING</td>
  <td>Average all elements in each pool. y = (x1 + x2 + … + xn) / pool_size;
So X[N C H W] reduces to Y[N C 1 1]

GlobalAveragePool(X):
InputShape = Shape(X)
SpatialDimensions = Slice(InputShape; starts=2) // skip leading N and C dimensions.
NewShape = Unsqueeze(SpatialDimensions, Const([0,1]) // Prepend with [1,1]
Output = AveragePool(X, kernel_shape=NewShape))</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Average Pooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#AveragePool">AveragePool</a></td>
  <td>DML_OPERATOR_AVERAGE_POOLING</td>
  <td>y = (x1 + x2 + … + xn) / pool_size</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Global Maximum Pooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#GlobalMaxPool">GlobalMaxPool</a></td>
  <td>DML_OPERATOR_MAX_POOLING with output being 1 element</td>
  <td>y = max(x1, x2, … x_pool_size)</td>
  <td>Exact</td>
 </tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Maximum Pooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#MaxPool">MaxPool</a></td>
  <td>DML_OPERATOR_MAX_POOLING</td>
  <td>y = max(x1, x2, … x_pool_size)</td>
  <td>Exact</td>
 </tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Maximum Unpooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#MaxUnpool">MaxUnpool</a></td>
  <td>---</td>
  <td>Opposite of MaxPool. Fill the output tensor of the given shape (either explicit or the input shape plus padding) with zeros, then write each value from the input tensor into the output tensor at the element offset from the corresponding indices array.</td>
  <td>Exact</td>
</tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Lebesgue Pooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#LpPool">LpPool</a></td>
  <td>DML_OPERATOR_LP_POOLING</td>
  <td>y = (x1^p + x2^p + ... + xn^p) ^ (1/p); y is reduced for each kernel</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Global Lebesgue Pooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#GlobalLpPool">GlobalLpPool</a></td>
  <td>DML_OPERATOR_LP_POOLING with output being 1 element</td>
  <td>y = (x1^p + x2^p + ... + xn^p) ^ (1/p);
      So X[N C H W] reduces to Y[N C 1 1]
      e.g. (3^2 + 4^2) ^ (1/2) = 5</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="poolingOperator">
  <td>Pooling</td>
  <td>Maximum Region of Interest Pooling</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#MaxRoiPool">MaxRoiPool</a></td>
  <td>DML_OPERATOR_ROI_POOLING (only POOLING_MAX is supported)</td>
  <td>Apply MaxPool to given input within each numbered region (batch_index, w_offset_start, h_offset_start, w_offset_last_inclusive, h_offset_last_inclusive), and write the maximal value back to the output.
      questions: Are x2 and y2 really supposed to be end-inclusive? If so, how can that possibly work correctly with the spatial_scale attribute? That's broken and inconsistent with a lot of graphics API's and Python array slice start:end notation. What's the point of the pooled_shape when each region has a specific size anyway?
  </td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Sum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceSum">ReduceSum</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_SUM</td>
  <td>Y.shape = If(keepdims==1, Shape(X), Squeeze(X, axes))
      y = (x1 + x2 + ... + xn)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Mean</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceMean">ReduceMean</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_AVERAGE</td>
  <td>y = (x1 + x2 + ... + xn) / n</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Product</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceProd">ReduceProd</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_MULTIPLY</td>
  <td>y = (x1 * x2 * ... * xn)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce to Logarithm of Sum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceLogSum">ReduceLogSum</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_LOG_SUM</td>
  <td>F(X) = Log(ReduceSum(X, axes, keepdims))
      or: y = logₑ(x1 + x2 + ... + xn)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Logarithm of Sum of Exponents</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceLogSumExp">ReduceLogSumExp</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_LOG_SUM_EXP</td>
  <td>F(X) = Log(ReduceSum(Exp(X), axes, keepdims))
      or: y = logₑ(expₑ(x1) + expₑ(x2) + ... + expₑ(xn))
  </td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Sum of Squares</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceSumSquare">ReduceSumSquare</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_SUM_SQUARE</td>
  <td>F(X) = ReduceSum(Pow(X, 2), axes, keepdims)
      or: y = x1^2 + x2^2 + ... + xn^2</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Sum of Absolute Values</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceL1">ReduceL1</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_L1</td>
  <td>F(X) = ReduceSum(Abs(X), axes, keepdims)
      or: y = abs(x1) + abs(x2) + ... + abs(xn)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To L2 Distance</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceL2">ReduceL2</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_L2</td>
  <td>F(X) = Sqrt(ReduceSum(Pow(X, 2), axes, keepdims))
  or: y = sqrt(x1^2 + x2^2 + ... + xn^2)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Maximum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceMax">ReduceMax</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_MAX</td>
  <td>x = max(max(max(x1, x2), x3), ..., xn)</td>
  <td>Exact</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Minimum</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ReduceMin">ReduceMin</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_MIN</td>
  <td>x = min(min(min(x1, x2), x3), ..., xn)</td>
  <td>Exact</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Maximum Argument</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ArgMax">ArgMax</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_ARGMAX</td>
  <td>int32 {i j k ..} = maxindex(X Y Z …)</td>
  <td>Exact</td>
 </tr>
 <tr class="reductionOperator">
  <td>Reduction</td>
  <td>Reduce To Minimum Argument</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#ArgMin">ArgMin</a></td>
  <td>DML_OPERATOR_REDUCE with DML_REDUCE_FUNCTION_ARGMIN</td>
  <td>int32 {i j k ..} = minindex(X Y Z …)</td>
  <td>Exact</td>
 </tr>
 <tr>
  <td>Imaging Operators</td>
  <td>Resample Up</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Upsample">Upsample</a></td>
  <td>DML_OPERATOR_UPSAMPLE_2D</td>
  <td>Y.Shape = Floor(Shape(X) * Scales)
      Y[output_i] = X[output_i / Scales]</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="controlFlowOperator">
  <td>Control Flow</td>
  <td>If</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#If">If</a></td>
  <td>---</td>
  <td>f(cond, then_graph, else_graph, outputs...):
      subgraph = cond ? then_graph : else_graph
      outputs = subgraph(implictly_named_inputs_from_outer_graph)
  </td>
  <td>Exact</td>
 </tr>
 <tr class="controlFlowOperator">
  <td>Control Flow</td>
  <td>Loop</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Loop">Loop</a></td>
  <td>---</td>
  <td>TODO:CODE</td>
  <td>Exact</td>
 </tr>
 <tr class="controlFlowOperator">
  <td>Control Flow</td>
  <td>Scan</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Scan">Scan</a></td>
  <td>---</td>
  <td>TODO:CODE</td>
  <td>Exact</td>
 </tr>
 <tr class="normalizationOperator">
  <td>Normalization</td>
  <td>Spatial Normalization</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#InstanceNormalization">InstanceNormalization</a></td>
  <td>DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION with acrossChannels=false, normalizeVariance=true, scale and bias provided<br/>DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION1 with all axes except the first two (N,C).</td>
  <td>InstanceNormalization(X, scale, bias) = scale * (X - mean) / sqrt(Variance + epsilon) + reshape(Bias, [batch size, C axis size, spatial dims...])

Mean and variance are computed across spatial dimensions DHW, independently per batch per channel NC:

axes = [2,3, ..., inputRank-1] // Exclude axes {0,1}
mean = (x0 + x1 + …) / xn;
variance = ((x0 - xmean)^2 + (x1 - xmean)^2 + …) / xn

ONNX:
mean = ReduceAverage(axes, keepdims=true)
variance = ReduceAverage(Pow(Sub(x, mean), 2), axes, keepdims=true)

NumPy:
mean = np.mean(x, axis=axes, keepdims=True)
variance = np.var(x, axis=axes, keepdims=True)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="normalizationOperator">
  <td>Normalization</td>
  <td>Batch Normalization</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#BatchNormalization">BatchNormalization</a></td>
  <td>DML_OPERATOR_BATCH_NORMALIZATION</td>
  <td>BatchNormalization(X, scale, bias, mean, var, epsilon, momentum) = Scale * (X - mean) / sqrt(Variance + epsilon) + reshape(Bias, C axis)

Statistics are precomputed across batch and spatial dimensions NDHW (and not just the batch dimension, as the name would misleadingly lead you believe) independently for each channel C, then reshaped to be input size compatible.

axes = [0,2, ..., inputRank-1] // Exclude axes {1}
mean = (x0 + x1 + …) / xn;
variance = ((x0 - xmean)^2 + (x1 - xmean)^2 + …) / xn

ONNX:
mean = ReduceAverage(axes, keepdims=true)
variance = ReduceAverage(Pow(Sub(x, mean), 2), axes, keepdims=true)

NumPy:
mean = np.var(x, axis=axes, keepdims=True)
variance = np.mean(x, axis=axes, keepdims=True)
</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="normalizationOperator">
  <td>Normalization</td>
  <td>Spatial&Channel Normalization</td>
  <td>NA</td>
  <td>DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION1 with all axes except the first batch (N).</td>
  <td>LayerNormalization(X, scale, bias) = scale * (X - mean) / sqrt(Variance + epsilon) + Bias

Mean and variance are computed across spatial&channel dimensions CDHW, independently per batch N:

axes = [1,2,3, ..., inputRank-1] // Exclude axes {0}
mean = (x0 + x1 + …) / xn;
variance = ((x0 - xmean)^2 + (x1 - xmean)^2 + …) / xn

ONNX:
mean = ReduceAverage(axes, keepdims=true)
variance = ReduceAverage(Pow(Sub(x, mean), 2), axes, keepdims=true)

NumPy:
mean = np.mean(x, axis=axes, keepdims=True)
variance = np.var(x, axis=axes, keepdims=True)</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="normalizationOperator">
  <td>Normalization</td>
  <td>Local Response Normalization</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#LRN">LRN</a></td>
  <td>DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION</td>
  <td>For each output element, sum all the corresponding inputs in a local window, considering scale, power, and bias.

LRN(x, localSize, scaleAlpha, powerBeta, bias) = x / (bias + (scaleAlpha / localSize) * sum(xi^2 for every xi in the local region)) ^ powerBeta
defaults: bias = 1

Decomposition:
Radius = LocalSize / 2
axes = if CrossChannel then {C} else {H,W}
PoolingWindowSizes = ConstructWindowSizes(axes, Rank, 1 + Radius) // dimension = 1 + Radius along axes, 1's elsewhere
Summed = PoolSum(Pow(Input, 2), axes, PoolingWindowSizes) * Alpha / LocalSize
Biased = Summed + Bias
Output = Input / Pow(Biased, Beta)
  </td>
  <td>Precision TBD</td>
 </tr>
 <tr class="normalizationOperator">
  <td>Normalization</td>
  <td>Mean Variance Normalization</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#MeanVarianceNormalization">MeanVarianceNormalization</a></td>
  <td>DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION</td>
  <td>For each output element, subtract the mean, and divide by standard deviation.

MeanVarianceNormalization(X) = (X - mean) / standardDeviation
MeanVarianceNormalization(X) = (X - mean) / sqrt(variance + epsilon)
MeanVarianceNormalization(X) = (X - mean(X)) / sqrt(mean((X - mean(X))^2))
MeanVarianceNormalization(X) = (X - mean(X)) / sqrt(mean(X^2) - mean(X)^2))

ONNX:
Exponent = Const(2.0)
Epsilon = Const(1e-9)
X_RM = ReduceMean(X)
EX_squared = Pow(X_RM, Exponent)
X_squared = Pow(X, Exponent)
E_Xsquared = ReduceMean(X_squared)
Variance = Sub(E_Xsquared, EX_squared)
STD = Sqrt(Variance)
X_variance = Sub(X, X_RM)
Processed_STD = Add(STD, Epsilon)
X_MVN = Div(X_variance, Processed_STD)

NumPy:
dataMean = np.mean(inputData, axes, keepdims=1)
dataMeanSquared = np.power(dataMean, 2)
dataSquared = np.power(inputData, 2)
dataSquareMeaned = np.mean(dataSquared, axes, keepdims=1)
std = np.sqrt(dataSquareMeaned - dataMeanSquared)
output = (inputData - dataMean) / (std + 1e-9)
</td>
  <td>Precision TBD</td>
 </tr>
 <tr class="normalizationOperator">
  <td>Normalization</td>
  <td>Lebesgue Length Normalization</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#LpNormalization">LpNormalization</a></td>
  <td>DML_OPERATOR_LP_NORMALIZATION</td>
  <td><code>f(x) = x / sqrt(variance(X) + epsilon)</code> TODO: verify formula
<code>aggregate F(X, p)
    // TODO: verify this. mean? variance not / n?
    X_reduced = ReduceLp(X, p, axis, keepdims=1)
    // where ReduceLP is ReduceL1 or ReduceL2 based on p
    X_reduced_broadcast = Expand(X_reduced, Shape(X))
    X_variance = Pow(X_reduced_broadcast, 2)
    Epsilon = Const(1e-9)
    Output = Div(X, Sqrt(Add(X_variance, Epsilon)))
endaggregate</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Sparse tensor collation</td>
  <td>Nonzero Indices List</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#NonZero">Nonzero</a></td>
  <td>---</td>
  <td>Append the coordinates of every nonzero input value to a list, with coordinates stored interleaved (so not [[X1,Y1,Z1], [X2,Y2,Z2], …] but rather [[X1,X2,…], [Y1,Y1,…], [Z1,Z2,…]]).</td>
  <td>Exact</td>
 </tr>
 <tr>
  <td>NGram</td>
  <td>Term Frequency Inverse Document Frequency Vectorizer</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.4.0/docs/Operators.md#tfidfvectorizer">TfldfVectorizer</a></td>
  <td>---</td>
  <td>Read input front to back, incrementing output histogram for each occurrence found of desired patterns. It's basically a word count algorithm with the output histogram size equalling the number of words in the dictionary to find.</td>
  <td>Exact</td>
 </tr>
 <tr>
  <td>Aggregate</td>
  <td>Recurrent Neural Network</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#RNN">RNN</a></td>
  <td>DML_OPERATOR_RNN</td>
  <td>Y = Activation(Clip(MatMul(X, Transpose(W)) + MatMul(Initial_h, Transpose(R)) + B), -clip, +clip)</td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Aggregate</td>
  <td>Gated Recurrent Unit</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#GRU">GRU</a></td>
  <td>DML_OPERATOR_GRU</td>
  <td>Iteratively apply matrix multiplication. TODO: Need better summary.
<code>
Z = Activation1(Clip(MatMul(X, Transpose(W1)) + MatMul(Initial_h1, Transpose(R1)) + b1, -clip, +clip))
R = Activation1(Clip(MatMul(X, Transpose(W2)) + MatMul(Initial_h1, Transpose(R2)) + b2, -clip, +clip))
C = Mul(Initial_h1, R)
O = Activation2(Clip(MatMul(X, Transpose(W3)) + MatMul(Initial_h1, Transpose(R3)) + b3, -clip, +clip))
Y = Mul((1-Z), O) + Mul(Z, Initial_h1)

W = [W1, W2, W3];
b1 = B[0, :] + B[3*hidden_size, :];
b2 = B[1, :] + B[4*hidden_size, :];
b3 = B[2, :] + B[5*hidden_size, :];</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Aggregate (Deleted)</td>
  <td>Gated Recurrent Unit Unit</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.0/docs/Operators.md#GRUUnit">GRUUnit</a></td>
  <td>NA, experimental</td>
  <td>???</td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Aggregate</td>
  <td>Long Short Term Memory</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#LSTM">LSTM</a></td>
  <td>DML_OPERATOR_LSTM</td>
  <td>Iteratively apply matrix multiplication. TODO: Need better summary.
<code>
I = Activation1f(Clip(MatMul(X, Transpose(W1)) + MatMul(Initial_h1, Transpose(R1)) + Mul(p, initial_c) + b1), -clip, +clip)
F = Activation1f(Clip(MatMul(X, Transpose(W2)) + MatMul(Initial_h1, Transpose(R2)) + Mul(p, initial_c) + b2), -clip, +clip)
Z = Activation2g(Clip(MatMul(X, Transpose(W3)) + MatMul(Initial_h1, Transpose(R3)) + b3), -clip, +clip)
C = Mul(Initial_h1, F) + Mul(I, Z)
O = Activation2g(clip(MatMul(X, Transpose(W4)) + MatMul(Initial_h1, Transpose(R4)) + Mul(p, initial_c) + b4))
Y = Mul(Activation3h(C), O)
&nbsp;
W = [W1, W2, W3, W4];
b1 = B[0, :] + B[4*hidden_size, :];
b2 = B[1, :] + B[5*hidden_size, :];
b3 = B[2, :] + B[6*hidden_size, :];
b4 = B[3, :] + B[7*hidden_size, :];</code></td>
  <td>Precision TBD</td>
 </tr>
 <tr>
  <td>Training</td>
  <td>Dropout</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Dropout">Dropout</a></td>
  <td>Identity actually (useful during training, not trained execution)</td>
  <td>For each element, randomly zero it or multiply it by 1 / (1 - ratio).
<code>F(X, ratio) = Where(Less(Random(0, 0.9999), ratio), 0, Mul(X, Recip(Sub(1, ratio)))),
or f(x) = iif(random(0, 0.9999) &lt ratio, 0, 1 / (1 - ratio) * x);</code>
For forward execution, ratio is 0, and so it's equivalent to: <code>F(X) = Identity(X)</code>
notes: If probability = 1, then all zeroes. If 0, then identity. Selected randomly per element.</td>
  <td>Unpredictable</td>
 </tr>
 <tr>
  <td>Deleted / Code Execution</td>
  <td>A TENsor Kernel</td>
  <td>ONNX <a href="https://github.com/onnx/onnx/blob/rel-1.0/docs/Operators.md#ATen">ATen</a></td>
  <td>Experimental to execute arbitrary Python code.</td>
  <td>???</td>
  <td>Unpredictable</td>
 </tr>
</table>

</body>
</html>

<!--

TODO: Add these

## ConvInteger

https://github.com/onnx/onnx/pull/1908

## MatMulInteger

https://github.com/onnx/onnx/pull/1908

## QLinearConv

https://github.com/onnx/onnx/pull/1908
## QLinearMatMul

https://github.com/onnx/onnx/pull/1908

## NonMaxSuppression

https://github.com/onnx/onnx/pull/1703
https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression  
tf.image.non_max_suppression
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cc

## StringNormalizer

https://github.com/onnx/onnx/pull/1745  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#StringNormalizer

## Resize (Resample=Upsample+Downsample)

https://github.com/onnx/onnx/pull/1773

Resample elements from the source to destination tensor, using the scale factors to compute the destination tensor size. Can use linear or nearest neighbor. Supports interpolation across multiple dimensions, not just 2D. So one can keep the same spatial size but interpolate across channels or across batches.

    samplesToInterpolate = 1 << rank
    samples.resize(samplesToInterpolate)

    for every outputCoordinate in output tensor
        // Offset the fractional component by the pixel center offsets.
        inputCoordinateFloat = (outputCoordinate + outputPixelOrigin) / scales
        inputCoordinateFractions = frac(inputCoordinateFloat) - inputPixelOrigin
        inputCoordinate = floor(inputCoordinateFloat)

        // If the lerp value is negative, adjust to lower pixel.
        inputCoordinate += where(inputCoordinateFractions < 0, -ones(rank), zeroes(rank))
        inputCoordinateFractions += where(inputCoordinateFractions < 0, ones(rank), zeroes(rank))

        // Read the surrounding samples from around the interpolated point.
        for sampleIndex = 0..<samplesToInterpolate
            inputCoordinate = clamp(SampleIndexToCoordinate(sampleIndex), zeroes(rank), input.shape - ones(rank))
            samples[sampleIndex] = input[inputCoordinate];
        endfor

        // Intepolate all pixels
        samplesLeftToInterpolate = samplesToInterpolate
        for axis = 0..<rank
            samplesLeftToInterpolate >>= 1
            for sampleIndex = 0..<samplesLeftToInterpolate
                samples[sampleIndex] = lerp(samples[sampleIndex], samples[sampleIndex + samplesLeftToInterpolate], inputCoordinateFractions[sampleIndex])
            endfor
        endfor

        output[outputCoordinate] = samples[0]
    endfor

## ThresholdedRelu (already supported)

https://github.com/onnx/onnx/pull/1856

    f(x) = if x > alpha then x else 0

## BitShift left/right elemntwise shift

https://github.com/onnx/onnx/pull/1931  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#BitShift

    2 << 3 = 8
    8 >> 3 = 2

    // transparently apply broadcasting...
    for each coordinate in output tensor
        if direction == "LEFT"
            output[coordinate] = X[coordinate] << Y[coordinate]
        else if direction == "RIGHT"
            output[coordinate] = X[coordinate] >> Y[coordinate]
        endif
        // else error
    endif

## CumSum cumulative summation

https://github.com/onnx/onnx/pull/2030  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#CumSum
tf.math.cumsum
https://www.tensorflow.org/api_docs/python/tf/math/cumsum

Tally the cumulative summation along a given axis.
output size = input size  
can proceed along any axis.  

    cumsum([a, b, c])  -> [a, a + b, a + b + c]
    cumsum([a, b, c], reverse=true)  -> [a + b + c, b + c, c]
    cumsum([a, b, c], exclusive=true)  -> [0, a, a + b]

    x = [[0,1,3],
         [2,3,5]]
    axis = 0
    exclusive = false
    reverse = false // so go down along y axis
    output = [[0, 1, 3],
              [2, 4, 8]]

Pseudocode:

    int numberOfElementsAlongAxis = input.dimensions[axis]
    int numberOfSlivers = ElementCount(input) / numberOfElementsAlongAxis

    for i = 0..<numberOfSlivers
        int tally = 0
        auto coordinates = ComputeCoordinates(input.dimensions, i, axis)

        for j = 0..<numberOfElementsAlongAxis
            coordinates[axis] = if reverse then numberOfElementsAlongAxis - j else j

            inputValue = input[coordinates]
            if !exclusive
                tally += input[coordinates]
            endif
            output[coordinates] = tally
            if exclusive
                tally += input[coordinates]
            endif
        endfor
    endfor

## Det determinant

https://github.com/onnx/onnx/pull/2233  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#Det

Once factored into upper and lower diagonal matrices, or into "row echelon form" with all the lower diagonal being zeroes, the determinant is the product of all values along the diagonal.

## DynamicQuantizeLinear

https://github.com/onnx/onnx/pull/2187  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#DynamicQuantizeLinear

Compute the minimum and maximum element in the input. Then apply QuantizeLinear, rounding halves to nearest evens.

    xmax = max(ReduceMax(X), 0)
    xmin = min(ReduceMin(X), 0)
    y_scale = (xmax - xmin) / (qmax - qmin) // qmax - qmin is just 255 for uint8
    intermediate_zero_point = (qmin - min(x)) / (qmax - qmin)
    y_zero_point = cast(round(saturate(itermediate_zero_point)))

    for i = 0..<numberOfElements
        output[i] = saturate(roundNearestEvens(input[i] / y_scale) + y_zero_point)
    endfor

## GatherElements

https://github.com/onnx/onnx/pull/2143  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#GatherElements

## GatherND

https://github.com/onnx/onnx/pull/2106  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#GatherND

## Range

https://github.com/onnx/onnx/pull/2242  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#Range

Generate a 1D increasing/decreasing sequence of numbers.

``` c
    numberOfElements = max(ceil((limit - start) / delta ), 0)
    output.shape = [numberOfElements]

    for i = 0..<numberOfElements
        output[i] = start + (i * delta);
    endfor
```

Example 1 Inputs: `start = 3, limit = 9, delta = 3 Output: [3, 6]`  
Example 2 Inputs: `start = 10, limit = 4, delta = -2 Output: [10, 8, 6]`

## Round

https://github.com/onnx/onnx/pull/2053  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#Round

For every element, round halves to nearest even.

``` c
    for i = 0..<numberOfElements
        output[i] = roundNearestEven(input[i]);
    endfor
```

## ScatterElements

https://github.com/onnx/onnx/pull/2143  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#ScatterElements

## ScatterND

https://github.com/onnx/onnx/pull/2220  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#ScatterND

## Unique

https://github.com/onnx/onnx/pull/2141  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#Unique

## ConcatFromSequence

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#ConcatFromSequence

## SequenceAt

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#SequenceAt

## SequenceConstruct

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#SequenceConstruct

## SequenceEmpty

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#SequenceEmpty

## SequenceErase

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#SequenceErase

## SequenceInsert

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#SequenceInsert

## SequenceLength

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#SequenceLength

## SplitToSequence

https://github.com/onnx/onnx/pull/2249  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#SplitToSequence

## Resize (with interpolation model)

https://github.com/onnx/onnx/pull/2057

## ReverseSequence

https://github.com/onnx/onnx/pull/1927


Gaussian Error Linear Units (GELUs)

https://arxiv.org/abs/1606.08415 GELU = 0.5*x*(1 + tanh[2/π(x + 0.044715 * x^3)])

https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/model_pytorch.py#L14-L15 def gelu(x):     return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))

https://github.com/microsoft/onnxruntime/pull/2293/files
    const T x = input[idx];
    const T in = (bias == nullptr) ? x : (x + bias[idx % bias_length]);
    const T cdf = a + a * Tanh(in * (c * in * in + b));
    output[idx] = in * cdf;

https://github.com/apache/incubator-mxnet/pull/14449/files

    const float GELU_CUBIC_CONSTANT = 0.044715;
    const float GELU_ROOT_2_OVER_PI = 0.7978845608028654;

    return 0.5 * x * f(x)
    f: return 1.0 + mx.nd.tanh(g(x))
    g: return ROOT_TWO_OVER_PI * (x + CUBE_CONSTANT * x * x * x)

## Mod

https://github.com/onnx/onnx/pull/1874  
https://github.com/onnx/onnx/blob/master/docs/Operators.md#Mod  
https://docs.scipy.org/doc/numpy/reference/generated/numpy.mod.html  
https://docs.scipy.org/doc/numpy/reference/generated/numpy.fmod.html  

numpy.fmod / numpy.mod

The 'fmod' attribute selects between Python vs C signs, which when set really means (a) truncate division toward zero (b) result's sign follows dividend.
Note HLSL says "The % operator is defined only in cases where either both sides are positive or both sides are negative", and so it's likely unsuitable to implement this operator. https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-operators

``` c
    // The C++11 specification says a % b always resolves to the sign of a:
    //
    //  x = [8,  8, -8, -8]
    //  y = [3, -3,  3, -3]
    //  z = [2,  2, -2, -2] -- C fmod(), C %, or numpy.fmod())
    //
    // In Python, a % b (assuming non-zero) has the same sign as b:
    //
    //  x = [8,  8, -8, -8]
    //  y = [3, -3,  3, -3]
    //  z = [2, -1,  1, -2] -- Python % or numpy.mod
    //
    Mod(x, y, fmod) = if fmod == 1 then return x - (y * TruncateTowardZero(x / y)) // like C
                                   else return x - (y * Floor(x / y)) // like Python
```

-->
